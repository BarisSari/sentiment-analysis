{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Vlad/anaconda3/envs/work_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle\n",
    "import string\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv('../toxicity/pos_phrases.csv', encoding='utf8')\n",
    "positive = positive['text'].fillna(\"fillna\").values\n",
    "\n",
    "negative = pd.read_csv('../toxicity/neg_phrases.csv', encoding='utf8')\n",
    "negative = negative['text'].fillna(\"fillna\").values\n",
    "y_pos = np.zeros((positive.shape[0], 1))\n",
    "y_neg = np.ones((negative.shape[0], 1))\n",
    "\n",
    "X = np.concatenate((positive,negative))\n",
    "y = np.concatenate((y_pos,y_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = []\n",
    "with open('../toxicity/X_f__1.txt', 'r') as f:\n",
    "        for i in f.readlines():\n",
    "            X1.append(i)          \n",
    "X1 = np.array(X1)\n",
    "X = np.concatenate((X,X1))\n",
    "y = np.tile(y,(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "phrases = []\n",
    "alph = [' ','ё','й','ц','у','к','е','н','г','ш','щ','з','х','ъ','ф','ы','в','а','п','р','о','л','д','ж','э','я','ч','с','м','и','т','ь','б','ю']\n",
    "alph = set(alph)\n",
    "for string in X:\n",
    "    string = ''.join(i if i in alph else ' ' for i in string.lower())\n",
    "    string = re.sub(' +',' ',string)\n",
    "    phrases.append(string.strip())\n",
    "X = phrases\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from num2words import num2words\n",
    "\n",
    "def clear_format(text):\n",
    "    text = text.lower()\n",
    "    text = [i for i in text.split() if i.isdigit or i.isalpsha()]\n",
    "    punctuation = \"\"\"!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~¹³²⓶\"\"\"\n",
    "    return ' '.join(text).translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "    \n",
    "def num_to_words(text):\n",
    "    buf = text.split()\n",
    "    for i, word in enumerate(buf):\n",
    "        if word.isdigit():\n",
    "            if len(word) > 5:\n",
    "                buf[i] = ''\n",
    "            else:\n",
    "                buf[i] = str(num2words(word))\n",
    "    return ' '.join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "        msk = np.random.rand(len(y)) < 0.9\n",
    "        y_train = y[msk]\n",
    "        y_test = y[~msk]\n",
    "        X_train = X[msk]\n",
    "        X_test = X[~msk]\n",
    "\n",
    "        X_train = [clear_format(num_to_words(clear_format(i))) for i in  X_train]\n",
    "        X_test = [clear_format(num_to_words(clear_format(i))) for i in  X_test]\n",
    "\n",
    "        max_features = 30000\n",
    "        maxlen = 100\n",
    "        embed_size = 300\n",
    "\n",
    "        tokenizer = text.Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "        X_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_test = tokenizer.texts_to_sequences(X_test)\n",
    "        x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "        x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 s, sys: 699 ms, total: 39.9 s\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test, tokenizer = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203095\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 300)\n",
      "CPU times: user 2min 32s, sys: 4.59 s, total: 2min 37s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBEDDING_FILE = 'cc.ru.300.vec'\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n",
    "\n",
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "print(embedding_matrix.shape)\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    9000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 300)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 256)    439296      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,482,497\n",
      "Trainable params: 9,482,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Bidirectional, LSTM, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(embed_size = 300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = max_features, \n",
    "                  output_dim = embed_size, weights=[embedding_matrix])(inp)\n",
    "    prefilt_x = Dropout(0.5)(x) \n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(prefilt_x)\n",
    "    x = Conv1D(64, kernel_size=2, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(x)\n",
    "#     x = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128 \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 408482 samples, validate on 45186 samples\n",
      "Epoch 1/10\n",
      "408482/408482 [==============================] - 1388s 3ms/step - loss: 0.6206 - acc: 0.6520 - val_loss: 0.5527 - val_acc: 0.7174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55268, saving model to rus_best_weights.h5\n",
      "Epoch 2/10\n",
      "408482/408482 [==============================] - 1384s 3ms/step - loss: 0.5578 - acc: 0.7131 - val_loss: 0.5323 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55268 to 0.53226, saving model to rus_best_weights.h5\n",
      "Epoch 3/10\n",
      "408482/408482 [==============================] - 1389s 3ms/step - loss: 0.5337 - acc: 0.7315 - val_loss: 0.5192 - val_acc: 0.7388\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53226 to 0.51921, saving model to rus_best_weights.h5\n",
      "Epoch 4/10\n",
      "408482/408482 [==============================] - 1394s 3ms/step - loss: 0.5180 - acc: 0.7425 - val_loss: 0.5125 - val_acc: 0.7428\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.51921 to 0.51248, saving model to rus_best_weights.h5\n",
      "Epoch 5/10\n",
      "408482/408482 [==============================] - 1391s 3ms/step - loss: 0.5061 - acc: 0.7511 - val_loss: 0.5092 - val_acc: 0.7454\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51248 to 0.50915, saving model to rus_best_weights.h5\n",
      "Epoch 6/10\n",
      "408482/408482 [==============================] - 1386s 3ms/step - loss: 0.4952 - acc: 0.7591 - val_loss: 0.5048 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50915 to 0.50483, saving model to rus_best_weights.h5\n",
      "Epoch 7/10\n",
      "408482/408482 [==============================] - 1391s 3ms/step - loss: 0.4866 - acc: 0.7636 - val_loss: 0.5020 - val_acc: 0.7501\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.50483 to 0.50200, saving model to rus_best_weights.h5\n",
      "Epoch 8/10\n",
      "408482/408482 [==============================] - 1396s 3ms/step - loss: 0.4783 - acc: 0.7686 - val_loss: 0.5000 - val_acc: 0.7524\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.50200 to 0.49999, saving model to rus_best_weights.h5\n",
      "Epoch 9/10\n",
      "408482/408482 [==============================] - 1393s 3ms/step - loss: 0.4708 - acc: 0.7735 - val_loss: 0.5000 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49999\n",
      "Epoch 10/10\n",
      "408482/408482 [==============================] - 1382s 3ms/step - loss: 0.4637 - acc: 0.7784 - val_loss: 0.4982 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49999 to 0.49815, saving model to rus_best_weights.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5c24aadd8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=\"rus_best_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early] \n",
    "model.fit(x_train, y_train, \n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle = True,\n",
    "          callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 408482 samples, validate on 45186 samples\n",
      "Epoch 1/10\n",
      "408482/408482 [==============================] - 1307s 3ms/step - loss: 0.2984 - acc: 0.8703 - val_loss: 0.5584 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76533, saving model to test-rus_best_weights.h5\n",
      "Epoch 2/10\n",
      "408482/408482 [==============================] - 1298s 3ms/step - loss: 0.3484 - acc: 0.8413 - val_loss: 0.5350 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76533 to 0.76856, saving model to test-rus_best_weights.h5\n",
      "Epoch 3/10\n",
      "408482/408482 [==============================] - 1309s 3ms/step - loss: 0.3367 - acc: 0.8469 - val_loss: 0.5438 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76856\n",
      "Epoch 4/10\n",
      "134784/408482 [========>.....................] - ETA: 15:56 - loss: 0.3216 - acc: 0.8551"
     ]
    }
   ],
   "source": [
    "model.load_weights('rus_best_weights.h5')\n",
    "file_path=\"test-rus_best_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early] \n",
    "model.fit(x_train, y_train, \n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle = True,\n",
    "          callbacks=callbacks_list)\n",
    "# score, acc = model.evaluate(x_test, y_test)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 300)    60928800    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, None, 300)    0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "word_fcl_0 (Conv1D)             (None, None, 16)     4816        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ngram_2_cnn_0 (Conv1D)          (None, None, 16)     14416       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ngram_4_cnn_0 (Conv1D)          (None, None, 16)     14416       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ngram_6_cnn_0 (Conv1D)          (None, None, 16)     14416       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ngram_8_cnn_0 (Conv1D)          (None, None, 16)     14416       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_fcl_1 (Conv1D)             (None, None, 32)     544         word_fcl_0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ngram_2_cnn_1 (Conv1D)          (None, None, 32)     1568        ngram_2_cnn_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ngram_4_cnn_1 (Conv1D)          (None, None, 32)     1568        ngram_4_cnn_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ngram_6_cnn_1 (Conv1D)          (None, None, 32)     1568        ngram_6_cnn_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ngram_8_cnn_1 (Conv1D)          (None, None, 32)     1568        ngram_8_cnn_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 32)           0           word_fcl_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 32)           0           ngram_2_cnn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 32)           0           ngram_4_cnn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 32)           0           ngram_6_cnn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 32)           0           ngram_8_cnn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32)           0           global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32)           0           global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32)           0           global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32)           0           global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32)           0           global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 160)          0           dropout_20[0][0]                 \n",
      "                                                                 dropout_21[0][0]                 \n",
      "                                                                 dropout_22[0][0]                 \n",
      "                                                                 dropout_23[0][0]                 \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64)           10304       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 64)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 32)           2080        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 32)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            33          dropout_26[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 61,010,513\n",
      "Trainable params: 61,010,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model2(conv_layers = 2, \n",
    "                dilation_rates = [0, 2, 4, 6, 8], \n",
    "                embed_size = 300):\n",
    "    inp = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer.word_index)+1, \n",
    "                  output_dim = embed_size)(inp)\n",
    "    prefilt_x = Dropout(0.5)(x)\n",
    "    out_conv = []\n",
    "    # dilation rate lets us use ngrams and skip grams to process \n",
    "    for dilation_rate in dilation_rates:\n",
    "        x = prefilt_x\n",
    "        for i in range(2):\n",
    "            if dilation_rate>0:\n",
    "                x = Conv1D(16*2**(i), \n",
    "                           kernel_size = 3, \n",
    "                           dilation_rate = dilation_rate,\n",
    "                          activation = 'relu',\n",
    "                          name = 'ngram_{}_cnn_{}'.format(dilation_rate, i)\n",
    "                          )(x)\n",
    "            else:\n",
    "                x = Conv1D(16*2**(i), \n",
    "                           kernel_size = 1,\n",
    "                          activation = 'relu',\n",
    "                          name = 'word_fcl_{}'.format(i))(x)\n",
    "        out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    x = concatenate(out_conv, axis = -1)    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model2 = build_model2()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 408482 samples, validate on 45186 samples\n",
      "Epoch 1/30\n",
      "408482/408482 [==============================] - 348s 852us/step - loss: 0.6113 - acc: 0.6577 - val_loss: 0.5435 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72478, saving model to rus_second_try_best_weights.h5\n",
      "Epoch 2/30\n",
      "408482/408482 [==============================] - 339s 830us/step - loss: 0.5265 - acc: 0.7389 - val_loss: 0.5219 - val_acc: 0.7368\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.72478 to 0.73682, saving model to rus_second_try_best_weights.h5\n",
      "Epoch 3/30\n",
      "408482/408482 [==============================] - 339s 830us/step - loss: 0.4944 - acc: 0.7614 - val_loss: 0.5167 - val_acc: 0.7418\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.73682 to 0.74180, saving model to rus_second_try_best_weights.h5\n",
      "Epoch 4/30\n",
      "408482/408482 [==============================] - 339s 829us/step - loss: 0.4680 - acc: 0.7784 - val_loss: 0.5154 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.74180 to 0.74326, saving model to rus_second_try_best_weights.h5\n",
      "Epoch 5/30\n",
      "408482/408482 [==============================] - 339s 831us/step - loss: 0.4419 - acc: 0.7946 - val_loss: 0.5161 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.74326 to 0.74680, saving model to rus_second_try_best_weights.h5\n",
      "Epoch 6/30\n",
      "408482/408482 [==============================] - 339s 830us/step - loss: 0.4142 - acc: 0.8102 - val_loss: 0.5267 - val_acc: 0.7471\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.74680 to 0.74709, saving model to rus_second_try_best_weights.h5\n",
      "Epoch 7/30\n",
      "408482/408482 [==============================] - 339s 830us/step - loss: 0.3874 - acc: 0.8253 - val_loss: 0.5388 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.74709\n",
      "Epoch 8/30\n",
      "408482/408482 [==============================] - 339s 831us/step - loss: 0.3612 - acc: 0.8388 - val_loss: 0.5567 - val_acc: 0.7456\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.74709\n",
      "Epoch 9/30\n",
      "338816/408482 [=======================>......] - ETA: 57s - loss: 0.3338 - acc: 0.8524"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2c6d041d244b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/work_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/work_env/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/work_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/work_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/work_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128 \n",
    "epochs = 30\n",
    "\n",
    "file_path=\"rus_second_try_best_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early] \n",
    "model2.fit(x_train, y_train, \n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle = True,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python work_env",
   "language": "python",
   "name": "work_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
